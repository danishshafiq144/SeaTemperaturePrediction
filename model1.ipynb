{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d544faeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 15:58:05,783 - INFO - Script execution started. Output will be saved in: Results_Model_Comparison_20250509_155805\n",
      "2025-05-09 15:58:05,788 - INFO - Loading and preprocessing dataset...\n",
      "2025-05-09 15:58:08,169 - INFO - Raw dataset shape: (515285, 37)\n",
      "2025-05-09 15:58:08,177 - INFO - \n",
      "First 5 rows before preprocessing:\n",
      "  station_id  longitude  latitude                 time  AtmosphericPressure  \\\n",
      "0         M1      -11.2   53.1266  2001-02-07 22:00:00                999.4   \n",
      "1         M1      -11.2   53.1266  2001-02-07 23:00:00               1000.0   \n",
      "2         M1      -11.2   53.1266  2001-02-08 00:00:00               1000.6   \n",
      "3         M1      -11.2   53.1266  2001-02-08 01:00:00               1001.2   \n",
      "4         M1      -11.2   53.1266  2001-02-08 02:00:00               1001.4   \n",
      "\n",
      "   WindDirection  WindSpeed  Gust  Wavelength  WavePeriod  ...  \\\n",
      "0           30.0      16.93  23.3         1.4         7.0  ...   \n",
      "1           40.0      15.95  25.3         1.4         7.0  ...   \n",
      "2           40.0      14.01  21.4         1.4         7.0  ...   \n",
      "3           30.0      14.01  19.5         1.4         7.0  ...   \n",
      "4           30.0      13.04  19.5         1.4         7.0  ...   \n",
      "\n",
      "   WindSpeed_rolling_3h  Pressure_rolling_6h  SeaTemperature_lag1  \\\n",
      "0             15.953333           997.300000                  9.1   \n",
      "1             16.276667           998.133333                  9.1   \n",
      "2             15.630000           998.933333                  9.1   \n",
      "3             14.656667           999.600000                  9.0   \n",
      "4             13.686667          1000.200000                  9.0   \n",
      "\n",
      "   SeaTemperature_lag2  SeaTemperature_rolling_3h  SeaTemperature_lag3  \\\n",
      "0                  9.1                   9.100000                  9.0   \n",
      "1                  9.1                   9.100000                  9.1   \n",
      "2                  9.1                   9.066667                  9.1   \n",
      "3                  9.1                   9.033333                  9.1   \n",
      "4                  9.0                   9.000000                  9.1   \n",
      "\n",
      "   SeaTemperature_lag6  SeaTemperature_rolling_6h  SeaTemperature_rolling_12h  \\\n",
      "0                  9.0                   9.066667                    9.050000   \n",
      "1                  9.1                   9.066667                    9.058333   \n",
      "2                  9.0                   9.066667                    9.058333   \n",
      "3                  9.0                   9.066667                    9.050000   \n",
      "4                  9.1                   9.050000                    9.050000   \n",
      "\n",
      "   SeaTemp_WindSpeed_interaction  \n",
      "0                        154.063  \n",
      "1                        145.145  \n",
      "2                        126.090  \n",
      "3                        126.090  \n",
      "4                        117.360  \n",
      "\n",
      "[5 rows x 37 columns]\n",
      "2025-05-09 15:58:08,275 - INFO - Missing values filled with column means.\n",
      "2025-05-09 15:58:08,347 - INFO - Preprocessed data sample saved to: Results_Model_Comparison_20250509_155805/preprocessed_data_sample.csv\n",
      "2025-05-09 15:58:08,366 - INFO - Creating sequences with sequence length: 24\n",
      "2025-05-09 15:58:09,181 - INFO - Sequence shapes - X: (515261, 24, 10), y: (515261, 1)\n",
      "2025-05-09 15:58:09,182 - INFO - Data split - LSTM Train shape: (412208, 24, 10), Test shape: (103053, 24, 10)\n",
      "2025-05-09 15:58:09,183 - INFO - Data split - RF/XGB Train shape: (412208, 10), Test shape: (103053, 10)\n",
      "2025-05-09 15:58:09,183 - INFO - Selected features: ['longitude', 'latitude', 'month_sin', 'HeatIndex', 'SeaTemperature_lag1', 'SeaTemperature_lag2', 'SeaTemperature_rolling_3h', 'SeaTemperature_rolling_6h', 'SeaTemperature_rolling_12h', 'SeaTemp_WindSpeed_interaction']\n",
      "2025-05-09 15:58:09,184 - INFO - Building LSTM model...\n",
      "2025-05-09 15:58:09,508 - INFO - LSTM model compiled and built successfully.\n",
      "2025-05-09 15:58:09,509 - INFO - Building RandomForestRegressor model...\n",
      "2025-05-09 15:58:09,510 - INFO - RandomForestRegressor model built successfully.\n",
      "2025-05-09 15:58:09,510 - INFO - Building XGBRegressor model...\n",
      "2025-05-09 15:58:09,511 - INFO - XGBRegressor model built successfully.\n",
      "2025-05-09 15:58:09,511 - INFO - Starting LSTM model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.00561, saving model to Results_Model_Comparison_20250509_155805\\best_lstm_sea_temperature_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 16:10:24,017 - WARNING - You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9662/9662 - 734s - 76ms/step - loss: 0.0043 - mae: 0.0506 - val_loss: 0.0056 - val_mae: 0.0616 - learning_rate: 1.0000e-03\n",
      "Epoch 2/200\n",
      "\n",
      "Epoch 2: val_loss improved from 0.00561 to 0.00374, saving model to Results_Model_Comparison_20250509_155805\\best_lstm_sea_temperature_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 16:21:53,849 - WARNING - You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9662/9662 - 690s - 71ms/step - loss: 0.0035 - mae: 0.0467 - val_loss: 0.0037 - val_mae: 0.0487 - learning_rate: 1.0000e-03\n",
      "Epoch 3/200\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.00374\n",
      "9662/9662 - 686s - 71ms/step - loss: 0.0035 - mae: 0.0460 - val_loss: 0.0052 - val_mae: 0.0584 - learning_rate: 1.0000e-03\n",
      "Epoch 4/200\n",
      "\n",
      "Epoch 4: val_loss improved from 0.00374 to 0.00345, saving model to Results_Model_Comparison_20250509_155805\\best_lstm_sea_temperature_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 16:47:39,920 - WARNING - You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9662/9662 - 860s - 89ms/step - loss: 0.0034 - mae: 0.0457 - val_loss: 0.0034 - val_mae: 0.0482 - learning_rate: 1.0000e-03\n",
      "Epoch 5/200\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.00345\n",
      "9662/9662 - 979s - 101ms/step - loss: 0.0034 - mae: 0.0454 - val_loss: 0.0035 - val_mae: 0.0480 - learning_rate: 1.0000e-03\n",
      "Epoch 6/200\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.00345\n",
      "9662/9662 - 964s - 100ms/step - loss: 0.0033 - mae: 0.0452 - val_loss: 0.0046 - val_mae: 0.0556 - learning_rate: 1.0000e-03\n",
      "Epoch 7/200\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.00345\n",
      "9662/9662 - 946s - 98ms/step - loss: 0.0033 - mae: 0.0450 - val_loss: 0.0035 - val_mae: 0.0480 - learning_rate: 1.0000e-03\n",
      "Epoch 8/200\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.00345\n",
      "9662/9662 - 940s - 97ms/step - loss: 0.0033 - mae: 0.0449 - val_loss: 0.0038 - val_mae: 0.0490 - learning_rate: 1.0000e-03\n",
      "Epoch 9/200\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.00345\n",
      "9662/9662 - 936s - 97ms/step - loss: 0.0033 - mae: 0.0448 - val_loss: 0.0038 - val_mae: 0.0487 - learning_rate: 1.0000e-03\n",
      "Epoch 10/200\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.00345\n",
      "9662/9662 - 931s - 96ms/step - loss: 0.0033 - mae: 0.0446 - val_loss: 0.0042 - val_mae: 0.0534 - learning_rate: 1.0000e-03\n",
      "Epoch 11/200\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.00345\n",
      "9662/9662 - 939s - 97ms/step - loss: 0.0033 - mae: 0.0445 - val_loss: 0.0035 - val_mae: 0.0480 - learning_rate: 1.0000e-03\n",
      "Epoch 12/200\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.00345\n",
      "9662/9662 - 930s - 96ms/step - loss: 0.0033 - mae: 0.0444 - val_loss: 0.0035 - val_mae: 0.0476 - learning_rate: 1.0000e-03\n",
      "Epoch 13/200\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.00345\n",
      "9662/9662 - 947s - 98ms/step - loss: 0.0032 - mae: 0.0443 - val_loss: 0.0036 - val_mae: 0.0481 - learning_rate: 1.0000e-03\n",
      "Epoch 14/200\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 14: val_loss improved from 0.00345 to 0.00344, saving model to Results_Model_Comparison_20250509_155805\\best_lstm_sea_temperature_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 19:25:10,449 - WARNING - You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9662/9662 - 940s - 97ms/step - loss: 0.0032 - mae: 0.0442 - val_loss: 0.0034 - val_mae: 0.0473 - learning_rate: 1.0000e-03\n",
      "Epoch 15/200\n",
      "\n",
      "Epoch 15: val_loss improved from 0.00344 to 0.00334, saving model to Results_Model_Comparison_20250509_155805\\best_lstm_sea_temperature_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 19:40:57,402 - WARNING - You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9662/9662 - 947s - 98ms/step - loss: 0.0032 - mae: 0.0437 - val_loss: 0.0033 - val_mae: 0.0471 - learning_rate: 5.0000e-04\n",
      "Epoch 16/200\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.00334\n",
      "9662/9662 - 911s - 94ms/step - loss: 0.0032 - mae: 0.0437 - val_loss: 0.0035 - val_mae: 0.0482 - learning_rate: 5.0000e-04\n",
      "Epoch 17/200\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.00334\n",
      "9662/9662 - 792s - 82ms/step - loss: 0.0032 - mae: 0.0436 - val_loss: 0.0037 - val_mae: 0.0488 - learning_rate: 5.0000e-04\n",
      "Epoch 18/200\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.00334\n",
      "9662/9662 - 985s - 102ms/step - loss: 0.0032 - mae: 0.0436 - val_loss: 0.0035 - val_mae: 0.0478 - learning_rate: 5.0000e-04\n",
      "Epoch 19/200\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.00334\n",
      "9662/9662 - 979s - 101ms/step - loss: 0.0032 - mae: 0.0436 - val_loss: 0.0036 - val_mae: 0.0486 - learning_rate: 5.0000e-04\n",
      "Epoch 20/200\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.00334\n",
      "9662/9662 - 988s - 102ms/step - loss: 0.0032 - mae: 0.0436 - val_loss: 0.0037 - val_mae: 0.0490 - learning_rate: 5.0000e-04\n",
      "Epoch 21/200\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.00334\n",
      "9662/9662 - 992s - 103ms/step - loss: 0.0032 - mae: 0.0436 - val_loss: 0.0035 - val_mae: 0.0476 - learning_rate: 5.0000e-04\n",
      "Epoch 22/200\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.00334\n",
      "9662/9662 - 1002s - 104ms/step - loss: 0.0031 - mae: 0.0435 - val_loss: 0.0042 - val_mae: 0.0522 - learning_rate: 5.0000e-04\n",
      "Epoch 23/200\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.00334\n",
      "9662/9662 - 995s - 103ms/step - loss: 0.0031 - mae: 0.0435 - val_loss: 0.0040 - val_mae: 0.0512 - learning_rate: 5.0000e-04\n",
      "Epoch 24/200\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.00334\n",
      "9662/9662 - 1011s - 105ms/step - loss: 0.0031 - mae: 0.0435 - val_loss: 0.0035 - val_mae: 0.0478 - learning_rate: 5.0000e-04\n",
      "Epoch 25/200\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.00334\n",
      "9662/9662 - 1116s - 116ms/step - loss: 0.0031 - mae: 0.0435 - val_loss: 0.0036 - val_mae: 0.0492 - learning_rate: 5.0000e-04\n",
      "Epoch 26/200\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.00334\n",
      "9662/9662 - 1161s - 120ms/step - loss: 0.0031 - mae: 0.0432 - val_loss: 0.0037 - val_mae: 0.0492 - learning_rate: 2.5000e-04\n",
      "Epoch 27/200\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.00334\n",
      "9662/9662 - 1156s - 120ms/step - loss: 0.0031 - mae: 0.0432 - val_loss: 0.0037 - val_mae: 0.0488 - learning_rate: 2.5000e-04\n",
      "Epoch 28/200\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.00334\n",
      "9662/9662 - 1093s - 113ms/step - loss: 0.0031 - mae: 0.0432 - val_loss: 0.0039 - val_mae: 0.0505 - learning_rate: 2.5000e-04\n",
      "Epoch 29/200\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.00334\n",
      "9662/9662 - 1131s - 117ms/step - loss: 0.0031 - mae: 0.0432 - val_loss: 0.0036 - val_mae: 0.0482 - learning_rate: 2.5000e-04\n",
      "Epoch 30/200\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.00334\n",
      "9662/9662 - 998s - 103ms/step - loss: 0.0031 - mae: 0.0432 - val_loss: 0.0036 - val_mae: 0.0487 - learning_rate: 2.5000e-04\n",
      "Epoch 31/200\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.00334\n",
      "9662/9662 - 759s - 79ms/step - loss: 0.0031 - mae: 0.0431 - val_loss: 0.0038 - val_mae: 0.0498 - learning_rate: 2.5000e-04\n",
      "Epoch 32/200\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.00334\n",
      "9662/9662 - 1088s - 113ms/step - loss: 0.0031 - mae: 0.0431 - val_loss: 0.0036 - val_mae: 0.0486 - learning_rate: 2.5000e-04\n",
      "Epoch 33/200\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.00334\n",
      "9662/9662 - 1219s - 126ms/step - loss: 0.0031 - mae: 0.0431 - val_loss: 0.0040 - val_mae: 0.0510 - learning_rate: 2.5000e-04\n",
      "Epoch 34/200\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.00334\n",
      "9662/9662 - 1249s - 129ms/step - loss: 0.0031 - mae: 0.0431 - val_loss: 0.0037 - val_mae: 0.0492 - learning_rate: 2.5000e-04\n",
      "Epoch 35/200\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.00334\n",
      "9662/9662 - 1268s - 131ms/step - loss: 0.0031 - mae: 0.0431 - val_loss: 0.0036 - val_mae: 0.0487 - learning_rate: 2.5000e-04\n",
      "Epoch 35: early stopping\n",
      "Restoring model weights from the end of the best epoch: 15.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 01:29:10,992 - INFO - Starting RandomForestRegressor training...\n",
      "2025-05-10 01:29:56,041 - INFO - Starting XGBRegressor training...\n",
      "2025-05-10 01:30:02,248 - INFO - Evaluating LSTM model...\n",
      "2025-05-10 01:32:14,230 - INFO - \n",
      "LSTM Test Set Regression Metrics:\n",
      "2025-05-10 01:32:14,231 - INFO - R² Score: 0.8917\n",
      "2025-05-10 01:32:14,233 - INFO - Mean Absolute Error: 0.6118\n",
      "2025-05-10 01:32:14,234 - INFO - Mean Absolute Percentage Error: 5.1706%\n",
      "2025-05-10 01:32:14,235 - INFO - Root Mean Squared Error: 0.7920\n",
      "2025-05-10 01:32:14,235 - INFO - Explained Variance Score: 0.8918\n",
      "2025-05-10 01:32:17,564 - INFO - LSTM visualizations saved successfully.\n",
      "2025-05-10 01:32:17,565 - INFO - Evaluating RandomForest model...\n",
      "2025-05-10 01:32:17,826 - INFO - \n",
      "RandomForest Test Set Regression Metrics:\n",
      "2025-05-10 01:32:17,827 - INFO - R² Score: 0.9720\n",
      "2025-05-10 01:32:17,829 - INFO - Mean Absolute Error: 0.2890\n",
      "2025-05-10 01:32:17,829 - INFO - Mean Absolute Percentage Error: 2.4422%\n",
      "2025-05-10 01:32:17,831 - INFO - Root Mean Squared Error: 0.4025\n",
      "2025-05-10 01:32:17,832 - INFO - Explained Variance Score: 0.9723\n",
      "2025-05-10 01:32:21,071 - INFO - RandomForest visualizations saved successfully.\n",
      "2025-05-10 01:32:21,072 - INFO - Evaluating XGBoost model...\n",
      "2025-05-10 01:32:21,248 - INFO - \n",
      "XGBoost Test Set Regression Metrics:\n",
      "2025-05-10 01:32:21,250 - INFO - R² Score: 0.9947\n",
      "2025-05-10 01:32:21,254 - INFO - Mean Absolute Error: 0.1076\n",
      "2025-05-10 01:32:21,256 - INFO - Mean Absolute Percentage Error: 0.9315%\n",
      "2025-05-10 01:32:21,257 - INFO - Root Mean Squared Error: 0.1759\n",
      "2025-05-10 01:32:21,259 - INFO - Explained Variance Score: 0.9948\n",
      "2025-05-10 01:32:24,285 - INFO - XGBoost visualizations saved successfully.\n",
      "2025-05-10 01:32:24,717 - INFO - LSTM training history plot saved.\n",
      "2025-05-10 01:32:24,718 - WARNING - You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "2025-05-10 01:32:24,877 - INFO - Models saved to: Results_Model_Comparison_20250509_155805\n",
      "2025-05-10 01:32:24,882 - INFO - Model comparison summary saved.\n",
      "2025-05-10 01:32:24,883 - INFO - Pipeline completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import (r2_score, mean_absolute_error, mean_absolute_percentage_error,\n",
    "                            mean_squared_error, explained_variance_score)\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==============================================\n",
    "# SETUP LOGGING AND OUTPUT FOLDER\n",
    "# ==============================================\n",
    "\n",
    "# Create output folder with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_folder = f'Results_Model_Comparison_{timestamp}'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(os.path.join(output_folder, 'execution.log')),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "logger.info(f\"Script execution started. Output will be saved in: {output_folder}\")\n",
    "\n",
    "# ==============================================\n",
    "# DATA LOADING AND PREPROCESSING\n",
    "# ==============================================\n",
    "\n",
    "def load_and_preprocess_data(data_path, target='SeaTemperature', selected_features=None):\n",
    "    \"\"\"Load and preprocess the dataset, selecting specified features and scaling data.\"\"\"\n",
    "    try:\n",
    "        logger.info(\"Loading and preprocessing dataset...\")\n",
    "\n",
    "        # Load the CSV file\n",
    "        df = pd.read_csv(data_path)\n",
    "        logger.info(f\"Raw dataset shape: {df.shape}\")\n",
    "        logger.info(\"\\nFirst 5 rows before preprocessing:\\n\" + str(df.head()))\n",
    "\n",
    "        # If selected_features is None, compute mutual information scores\n",
    "        if selected_features is None:\n",
    "            logger.info(\"No selected features provided. Computing mutual information scores...\")\n",
    "            features = [col for col in df.columns if col != target and df[col].dtype != 'object']\n",
    "            X = df[features].fillna(df[features].mean())\n",
    "            y = df[target]\n",
    "            \n",
    "            # Calculate mutual information scores\n",
    "            mi_scores = mutual_info_regression(X, y)\n",
    "            mi_scores_df = pd.DataFrame({'Feature': features, 'MI_Score': mi_scores})\n",
    "            mi_scores_df = mi_scores_df.sort_values(by='MI_Score', ascending=False)\n",
    "            \n",
    "            # Select top 10 features\n",
    "            selected_features = mi_scores_df.head(10)['Feature'].tolist()\n",
    "            logger.info(\"Top 10 features based on MI scores:\\n\" + str(mi_scores_df.head(10)))\n",
    "\n",
    "        # Ensure selected_features is a list\n",
    "        if not isinstance(selected_features, list):\n",
    "            raise ValueError(\"selected_features must be a list of feature names\")\n",
    "\n",
    "        # Verify all selected features exist in the dataset\n",
    "        missing_features = [f for f in selected_features if f not in df.columns]\n",
    "        if missing_features:\n",
    "            raise ValueError(f\"Features not found in dataset: {missing_features}\")\n",
    "\n",
    "        # Handle missing values by filling with column mean\n",
    "        df[selected_features] = df[selected_features].fillna(df[selected_features].mean())\n",
    "        logger.info(\"Missing values filled with column means.\")\n",
    "\n",
    "        # Split features and target\n",
    "        X = df[selected_features].values\n",
    "        y = df[target].values\n",
    "\n",
    "        # Scale features and target\n",
    "        scaler_X = MinMaxScaler()\n",
    "        scaler_y = MinMaxScaler()\n",
    "        X_scaled = scaler_X.fit_transform(X)\n",
    "        y_scaled = scaler_y.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "        # Save preprocessed data sample\n",
    "        preprocessed_sample = pd.DataFrame(X_scaled, columns=selected_features)\n",
    "        preprocessed_sample['target_scaled'] = y_scaled\n",
    "        preprocessed_sample.head(20).to_csv(os.path.join(output_folder, 'preprocessed_data_sample.csv'), index=False)\n",
    "        logger.info(f\"Preprocessed data sample saved to: {output_folder}/preprocessed_data_sample.csv\")\n",
    "\n",
    "        return X_scaled, y_scaled, scaler_X, scaler_y, selected_features\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during data loading/preprocessing: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "# ==============================================\n",
    "# SEQUENCE CREATION\n",
    "# ==============================================\n",
    "\n",
    "def create_sequences(X, y, seq_length=24):\n",
    "    \"\"\"Create sequences for LSTM input.\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Creating sequences with sequence length: {seq_length}\")\n",
    "        X_seq, y_seq = [], []\n",
    "        for i in range(len(X) - seq_length):\n",
    "            X_seq.append(X[i:i + seq_length])\n",
    "            y_seq.append(y[i + seq_length])\n",
    "        X_seq, y_seq = np.array(X_seq), np.array(y_seq)\n",
    "        logger.info(f\"Sequence shapes - X: {X_seq.shape}, y: {y_seq.shape}\")\n",
    "        return X_seq, y_seq\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during sequence creation: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "# ==============================================\n",
    "# MODEL BUILDING\n",
    "# ==============================================\n",
    "\n",
    "def build_lstm_model(input_shape):\n",
    "    \"\"\"Build an improved LSTM model with Bidirectional layers for regression.\"\"\"\n",
    "    try:\n",
    "        logger.info(\"Building LSTM model...\")\n",
    "        model = Sequential([\n",
    "            Bidirectional(LSTM(256, activation='tanh', input_shape=input_shape, return_sequences=True)),\n",
    "            Dropout(0.2),\n",
    "            Bidirectional(LSTM(128, activation='tanh', return_sequences=True)),\n",
    "            Dropout(0.2),\n",
    "            Bidirectional(LSTM(64, activation='tanh')),\n",
    "            Dropout(0.2),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        \n",
    "        model.compile(optimizer=RMSprop(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "        model.build(input_shape=(None, input_shape[0], input_shape[1]))\n",
    "        logger.info(\"LSTM model compiled and built successfully.\")\n",
    "        return model\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during model building: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "def build_rf_model():\n",
    "    \"\"\"Build RandomForestRegressor model.\"\"\"\n",
    "    try:\n",
    "        logger.info(\"Building RandomForestRegressor model...\")\n",
    "        model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "        logger.info(\"RandomForestRegressor model built successfully.\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during RF model building: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "def build_xgb_model():\n",
    "    \"\"\"Build XGBRegressor model.\"\"\"\n",
    "    try:\n",
    "        logger.info(\"Building XGBRegressor model...\")\n",
    "        model = XGBRegressor(n_estimators=100, max_depth=10, learning_rate=0.1, random_state=42, n_jobs=-1)\n",
    "        logger.info(\"XGBRegressor model built successfully.\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during XGB model building: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "# ==============================================\n",
    "# MODEL EVALUATION AND VISUALIZATION\n",
    "# ==============================================\n",
    "\n",
    "def evaluate_and_visualize(model, model_name, X_test, y_test, scaler_y, output_folder, is_lstm=False):\n",
    "    \"\"\"Evaluate the model and generate visualizations for regression.\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Evaluating {model_name} model...\")\n",
    "\n",
    "        # Predict on test set\n",
    "        if is_lstm:\n",
    "            y_pred = model.predict(X_test, verbose=0)\n",
    "        else:\n",
    "            X_test_flat = X_test.reshape(X_test.shape[0], -1) if is_lstm else X_test\n",
    "            y_pred = model.predict(X_test_flat)\n",
    "\n",
    "        # Reshape predictions to 2D if they're 1D\n",
    "        if len(y_pred.shape) == 1:\n",
    "            y_pred = y_pred.reshape(-1, 1)\n",
    "        if len(y_test.shape) == 1:\n",
    "            y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "        # Inverse transform predictions and actual values\n",
    "        y_test_inv = scaler_y.inverse_transform(y_test)\n",
    "        y_pred_inv = scaler_y.inverse_transform(y_pred)\n",
    "\n",
    "        # Regression metrics\n",
    "        r2 = r2_score(y_test_inv, y_pred_inv)\n",
    "        mae = mean_absolute_error(y_test_inv, y_pred_inv)\n",
    "        mape = mean_absolute_percentage_error(y_test_inv, y_pred_inv) * 100\n",
    "        rmse = np.sqrt(mean_squared_error(y_test_inv, y_pred_inv))\n",
    "        explained_var = explained_variance_score(y_test_inv, y_pred_inv)\n",
    "\n",
    "        # Log metrics\n",
    "        logger.info(f\"\\n{model_name} Test Set Regression Metrics:\")\n",
    "        logger.info(f\"R² Score: {r2:.4f}\")\n",
    "        logger.info(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "        logger.info(f\"Mean Absolute Percentage Error: {mape:.4f}%\")\n",
    "        logger.info(f\"Root Mean Squared Error: {rmse:.4f}\")\n",
    "        logger.info(f\"Explained Variance Score: {explained_var:.4f}\")\n",
    "\n",
    "        # Save metrics to file\n",
    "        with open(os.path.join(output_folder, f'{model_name}_metrics.txt'), 'w') as f:\n",
    "            f.write(f\"{model_name} Regression Metrics:\\n\")\n",
    "            f.write(f\"R² Score: {r2:.4f}\\n\")\n",
    "            f.write(f\"Mean Absolute Error: {mae:.4f}\\n\")\n",
    "            f.write(f\"Mean Absolute Percentage Error: {mape:.4f}%\\n\")\n",
    "            f.write(f\"Root Mean Squared Error: {rmse:.4f}\\n\")\n",
    "            f.write(f\"Explained Variance Score: {explained_var:.4f}\\n\")\n",
    "\n",
    "        # Visualizations\n",
    "        # 1. Predicted vs Actual Plot\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(y_test_inv, label='Actual Sea Temperature', alpha=0.7)\n",
    "        plt.plot(y_pred_inv, label='Predicted Sea Temperature', alpha=0.7)\n",
    "        plt.title(f'{model_name}: Actual vs Predicted Sea Temperature')\n",
    "        plt.xlabel('Sample Index')\n",
    "        plt.ylabel('Sea Temperature (°C)')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_folder, f'{model_name}_actual_vs_predicted.png'))\n",
    "        plt.close()\n",
    "\n",
    "        # 2. Error Distribution Plot\n",
    "        errors = y_test_inv - y_pred_inv\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(errors, kde=True, bins=30)\n",
    "        plt.title(f'{model_name}: Prediction Error Distribution')\n",
    "        plt.xlabel('Prediction Error (°C)')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_folder, f'{model_name}_error_distribution.png'))\n",
    "        plt.close()\n",
    "\n",
    "        # 3. Scatter Plot of Predicted vs Actual\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.scatter(y_test_inv, y_pred_inv, alpha=0.5)\n",
    "        plt.plot([y_test_inv.min(), y_test_inv.max()], [y_test_inv.min(), y_test_inv.max()], 'r--', lw=2)\n",
    "        plt.title(f'{model_name}: Predicted vs Actual Sea Temperature')\n",
    "        plt.xlabel('Actual Sea Temperature (°C)')\n",
    "        plt.ylabel('Predicted Sea Temperature (°C)')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_folder, f'{model_name}_scatter_pred_vs_actual.png'))\n",
    "        plt.close()\n",
    "\n",
    "        logger.info(f\"{model_name} visualizations saved successfully.\")\n",
    "\n",
    "        return {\n",
    "            'r2': r2, 'mae': mae, 'mape': mape, 'rmse': rmse, 'explained_var': explained_var\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during {model_name} evaluation/visualization: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "# ==============================================\n",
    "# MAIN EXECUTION\n",
    "# ==============================================\n",
    "\n",
    "def main(data_path, selected_features=None):\n",
    "    \"\"\"Main function to train and evaluate LSTM, RandomForest, and XGBoost models.\"\"\"\n",
    "    try:\n",
    "        # Step 1: Load and preprocess data\n",
    "        X_scaled, y_scaled, scaler_X, scaler_y, feature_cols = load_and_preprocess_data(\n",
    "            data_path, target='SeaTemperature', selected_features=selected_features\n",
    "        )\n",
    "\n",
    "        # Step 2: Create sequences for LSTM\n",
    "        seq_length = 24\n",
    "        X_seq, y_seq = create_sequences(X_scaled, y_scaled, seq_length)\n",
    "\n",
    "        # Step 3: Train-test split (80-20)\n",
    "        train_size = int(0.8 * len(X_seq))\n",
    "        X_train_seq, X_test_seq = X_seq[:train_size], X_seq[train_size:]\n",
    "        y_train_seq, y_test_seq = y_seq[:train_size], y_seq[train_size:]\n",
    "        \n",
    "        # For RF and XGB, use flattened data\n",
    "        X_train_flat = X_scaled[:train_size]\n",
    "        X_test_flat = X_scaled[train_size:len(X_scaled)-seq_length]\n",
    "        y_train_flat = y_scaled[:train_size]\n",
    "        y_test_flat = y_scaled[train_size:len(y_scaled)-seq_length]\n",
    "        \n",
    "        logger.info(\"Data split - LSTM Train shape: {}, Test shape: {}\".format(X_train_seq.shape, X_test_seq.shape))\n",
    "        logger.info(\"Data split - RF/XGB Train shape: {}, Test shape: {}\".format(X_train_flat.shape, X_test_flat.shape))\n",
    "        logger.info(f\"Selected features: {feature_cols}\")\n",
    "\n",
    "        # Step 4: Build models\n",
    "        lstm_model = build_lstm_model((seq_length, len(feature_cols)))\n",
    "        rf_model = build_rf_model()\n",
    "        xgb_model = build_xgb_model()\n",
    "\n",
    "        # Step 5: Train LSTM model with callbacks\n",
    "        lstm_callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1),\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6, verbose=1),\n",
    "            ModelCheckpoint(os.path.join(output_folder, 'best_lstm_sea_temperature_model.h5'),\n",
    "                        monitor='val_loss', save_best_only=True, verbose=1)\n",
    "        ]\n",
    "\n",
    "        logger.info(\"Starting LSTM model training...\")\n",
    "        lstm_history = lstm_model.fit(\n",
    "            X_train_seq, y_train_seq,\n",
    "            epochs=200,\n",
    "            batch_size=32,\n",
    "            validation_split=0.25,\n",
    "            callbacks=lstm_callbacks,\n",
    "            verbose=2\n",
    "        )\n",
    "\n",
    "        # Step 6: Train RF and XGB models\n",
    "        logger.info(\"Starting RandomForestRegressor training...\")\n",
    "        rf_model.fit(X_train_flat, y_train_flat.ravel())\n",
    "\n",
    "        logger.info(\"Starting XGBRegressor training...\")\n",
    "        xgb_model.fit(X_train_flat, y_train_flat.ravel())\n",
    "\n",
    "        # Step 7: Evaluate and visualize\n",
    "        lstm_metrics = evaluate_and_visualize(lstm_model, 'LSTM', X_test_seq, y_test_seq, scaler_y, output_folder, is_lstm=True)\n",
    "        rf_metrics = evaluate_and_visualize(rf_model, 'RandomForest', X_test_flat, y_test_flat, scaler_y, output_folder)\n",
    "        xgb_metrics = evaluate_and_visualize(xgb_model, 'XGBoost', X_test_flat, y_test_flat, scaler_y, output_folder)\n",
    "\n",
    "        # Step 8: Plot training history for LSTM\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(lstm_history.history['loss'], label='Training Loss')\n",
    "        plt.plot(lstm_history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title('LSTM: Training and Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(lstm_history.history['mae'], label='Training MAE')\n",
    "        plt.plot(lstm_history.history['val_mae'], label='Validation MAE')\n",
    "        plt.title('LSTM: Training and Validation MAE')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('MAE')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_folder, 'lstm_training_history.png'))\n",
    "        plt.close()\n",
    "\n",
    "        logger.info(\"LSTM training history plot saved.\")\n",
    "\n",
    "        # Step 9: Save models\n",
    "        lstm_model.save(os.path.join(output_folder, 'final_lstm_sea_temperature_model.h5'))\n",
    "        joblib.dump(rf_model, os.path.join(output_folder, 'final_rf_sea_temperature_model.pkl'))\n",
    "        joblib.dump(xgb_model, os.path.join(output_folder, 'final_xgb_sea_temperature_model.pkl'))\n",
    "        logger.info(f\"Models saved to: {output_folder}\")\n",
    "\n",
    "        # Step 10: Save comparison summary\n",
    "        comparison_df = pd.DataFrame({\n",
    "            'Metric': ['R²', 'MAE', 'MAPE (%)', 'RMSE', 'Explained Variance'],\n",
    "            'LSTM': [lstm_metrics['r2'], lstm_metrics['mae'], lstm_metrics['mape'], lstm_metrics['rmse'], lstm_metrics['explained_var']],\n",
    "            'RandomForest': [rf_metrics['r2'], rf_metrics['mae'], rf_metrics['mape'], rf_metrics['rmse'], rf_metrics['explained_var']],\n",
    "            'XGBoost': [xgb_metrics['r2'], xgb_metrics['mae'], xgb_metrics['mape'], xgb_metrics['rmse'], xgb_metrics['explained_var']]\n",
    "        })\n",
    "        comparison_df.to_csv(os.path.join(output_folder, 'model_comparison.csv'), index=False)\n",
    "        logger.info(\"Model comparison summary saved.\")\n",
    "\n",
    "        logger.info(\"Pipeline completed successfully.\")\n",
    "\n",
    "        return lstm_model, rf_model, xgb_model, lstm_history, scaler_y, feature_cols\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in main execution: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define data path\n",
    "    data_path = r'download_35.csv'\n",
    "\n",
    "    # Define selected features from RFE\n",
    "    selected_features = [\n",
    "        'longitude', 'latitude', 'month_sin', 'HeatIndex', 'SeaTemperature_lag1', \n",
    "        'SeaTemperature_lag2', 'SeaTemperature_rolling_3h', 'SeaTemperature_rolling_6h', \n",
    "        'SeaTemperature_rolling_12h', 'SeaTemp_WindSpeed_interaction'\n",
    "    ]\n",
    "\n",
    "    # Run the main function\n",
    "    lstm_model, rf_model, xgb_model, lstm_history, scaler_y, feature_cols = main(data_path, selected_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
